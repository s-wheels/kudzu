{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This section examines the different methods of examining models and where they might be applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall = Sensitivity =  TPR = \\frac{TP}{TP + FN}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Specificity = 1 - FPR = \\frac{TP}{TP + FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\end{equation*}\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "#### Metric Table\n",
    "\n",
    "<img src=\"imgs/metric_table.png\" style=\"width:800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs\n",
    "####Â ROC\n",
    "\n",
    "A Receiver Operating Characteristic Curve (ROC) plots the True Positive Rate (TPR) vs the False Positive Rate (FPR)\n",
    "\n",
    "<img src=\"imgs/roc-curve.png\" style=\"width:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC is a curve of probability, considering the distributions a classifier may appear like:\n",
    "\n",
    "<img src=\"imgs/prob_dist.png\" style=\"width:500px\"/>\n",
    "\n",
    "##### Perfect Classifier\n",
    "\n",
    "\n",
    "In a perfect classifier, where the TPR = 1 at FPR = 0, the two distributions would have no overlap. For most problems the distributions would look similar to above.\n",
    "\n",
    "\n",
    "<img src=\"imgs/perfect_classifier.png\" style=\"width:800px\"/>\n",
    "\n",
    "##### Worst Classifier\n",
    "\n",
    "In the worst possible classifier, the two distributions would be identical, with compelete overlap. This indicates that the model has no ability to discriminate between the two inputs.\n",
    "\n",
    "<img src=\"imgs/worst_classifier.png\" style=\"width:800px\"/>\n",
    "\n",
    "\n",
    "##### Perfectly Inverted Classifier\n",
    "\n",
    "You may expect that this would instead be the worst classifier, and in performance terms this is true as the model classifies everything wrong with 100% certainty. However for a binary classification problem this is in fact equivalent to the perfect soluton, as the output can simply be inverted to be 100% correct.\n",
    "\n",
    "<img src=\"imgs/perfectly_inverted_classifier.png\" style=\"width:800px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold\n",
    "\n",
    "The threshold can be adjusted and tuned to deliver the most  'useful' model, depending on what metric is the most important and what practical trade-offs are acceptable for your real-world problem.\n",
    "\n",
    "The metrics have an intimate relationship to one another, guided by the threshold.\n",
    "\n",
    "Sensitivity INCREASES, Specificity DECREASES\n",
    "\n",
    "Specificity DECREASES, Sensitivity INCREASES\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = Threshold\n",
    "\\end{equation}\n",
    "\n",
    "As the threshold increases: \n",
    "\\begin{equation}\n",
    "\\lim_{\\theta \\to 1} Specificity \\to 1 \\space \\space and \\space \\space\n",
    "Sensitivity \\to 0 \n",
    "\\end{equation}\n",
    "\n",
    "As the threshold decreases: \n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{\\theta \\to 0} Specificity \\to 0 \\space \\space and \\space \\space\n",
    "Sensitivity \\to 1 \n",
    "\\end{equation}\n",
    "\n",
    "As the threshold increases: \n",
    "\\begin{equation}\n",
    "\\lim_{\\theta \\to 1} TPR \\to 1 \\space \\space and \\space \\space\n",
    "FPR \\to 1\n",
    "\\end{equation}\n",
    "\n",
    "As the threshold decreases: \n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{\\theta \\to 0} TPR \\to 0 \\space \\space and \\space \\space\n",
    "FPR \\to 0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Multi-class ROC Curves\n",
    "\n",
    "For a multi-class model, you can use ROCs with a One vs All methodology. I.e. given 3 classes X, Y, Z you can have one ROC for X vs Y, Z, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Curves (Precision vs Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision vs recall curves can also be very important in situations where ROC may fall short. For example, in a problem with an extreme class imbalance, the ROC may fail to show the poor performance of the model but a PR curve would make this obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/roc_vs_prrc.png\" style=\"width:600px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
